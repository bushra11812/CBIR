{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cbirAUtoencoder.ipynb",
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1b28J3OT_2m5B72DJXV9S10qBguYNnz7g",
      "authorship_tag": "ABX9TyMNeJNROAU46rJAS7SjK7Kq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bushra11812/CBIR/blob/master/cbirAUtoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcPbyPgv610a"
      },
      "outputs": [],
      "source": [
        "!pip install barbar torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import pickle\n",
        "from barbar import Bar\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import cv2\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision import transforms\n",
        "#from torchsummary import summary\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import gc\n",
        "RANDOMSTATE = 0\n",
        "import os"
      ],
      "metadata": {
        "id": "xsot974G85jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ngEX6w248_xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = Path('//content/drive/MyDrive/datasets/ddt/')\n",
        "df = pd.DataFrame()\n",
        "df['image'] = [f for f in os.listdir(datasetPath) if os.path.isfile(os.path.join(datasetPath, f))]\n",
        "df['image'] = '//content/drive/MyDrive/datasets/ddt/' + df['image'].astype(str)\n",
        "\n"
      ],
      "metadata": {
        "id": "HP35xd8nAY4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBIRDataset(Dataset):\n",
        "    def __init__(self, dataFrame):\n",
        "        self.dataFrame = dataFrame\n",
        "        \n",
        "        self.transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "    \n",
        "    def __getitem__(self, key):\n",
        "        if isinstance(key, slice):\n",
        "            raise NotImplementedError('slicing is not supported')\n",
        "        \n",
        "        row = self.dataFrame.iloc[key]\n",
        "        image = self.transformations(Image.open(row['image']).convert('RGB'))\n",
        "        return image\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataFrame.index)"
      ],
      "metadata": {
        "id": "uJBmGHCAC0e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(DF):\n",
        "    trainDF, validateDF = train_test_split(DF, test_size=0.15, random_state=RANDOMSTATE)\n",
        "    train_set = CBIRDataset(trainDF)\n",
        "    validate_set = CBIRDataset(validateDF)\n",
        "    \n",
        "    return train_set, validate_set"
      ],
      "metadata": {
        "id": "cgtecxLVDTWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
        "            \n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=16, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=3, \n",
        "                      padding=1),  # (32,16,171,171)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2),  # (N,16,85,85)\n",
        "            \n",
        "            nn.Conv2d(in_channels=16, \n",
        "                      out_channels=8, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=2, \n",
        "                      padding=1),  # (N,8,43,43)\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=1)  # (N,8,42,42)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels = 8, \n",
        "                               out_channels=16, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2),  # (N,16,85,85)\n",
        "            nn.ReLU(True),\n",
        " \n",
        "            nn.ConvTranspose2d(in_channels=16, \n",
        "                               out_channels=8, \n",
        "                               kernel_size=(5,5), \n",
        "                               stride=3, \n",
        "                               padding=1),  # (N,8,255,255)\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=8, \n",
        "                               out_channels=3, \n",
        "                               kernel_size=(6,6), \n",
        "                               stride=2, \n",
        "                               padding=1),  # (N,3,512,512)\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zUO7vJUcDjvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvAutoencoder_v2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder_v2, self).__init__()\n",
        "        self.encoder = nn.Sequential(# in- (N,3,512,512)\n",
        "            \n",
        "            nn.Conv2d(in_channels=3, \n",
        "                      out_channels=64, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=64, \n",
        "                      out_channels=64, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), \n",
        "            \n",
        "            nn.Conv2d(in_channels=64, \n",
        "                      out_channels=128, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=2, \n",
        "                      padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=128, \n",
        "                      out_channels=128, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=0), \n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2), \n",
        "            \n",
        "            nn.Conv2d(in_channels=128, \n",
        "                      out_channels=256, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=2, \n",
        "                      padding=1), \n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256, \n",
        "                      out_channels=256, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1), \n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=256, \n",
        "                      out_channels=256, \n",
        "                      kernel_size=(3,3), \n",
        "                      stride=1, \n",
        "                      padding=1), \n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2, stride=2) \n",
        "        )\n",
        "#         self.summary()\n",
        "        self.decoder = nn.Sequential(\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels = 256, \n",
        "                               out_channels=256, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=1,\n",
        "                              padding=1), \n",
        " \n",
        "            nn.ConvTranspose2d(in_channels=256, \n",
        "                               out_channels=256, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=1, \n",
        "                               padding=1),  \n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(in_channels=256, \n",
        "                               out_channels=128, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=0),  \n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=128, \n",
        "                               out_channels=64, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=1),  \n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(in_channels=64, \n",
        "                               out_channels=32, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=1), \n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=32, \n",
        "                               out_channels=32, \n",
        "                               kernel_size=(3,3), \n",
        "                               stride=2, \n",
        "                               padding=1),  \n",
        "            nn.ReLU(True),\n",
        "            \n",
        "            nn.ConvTranspose2d(in_channels=32, \n",
        "                               out_channels=3, \n",
        "                               kernel_size=(4,4), \n",
        "                               stride=2, \n",
        "                               padding=2),  \n",
        "            nn.Tanh()\n",
        "        )\n",
        "#         self.summary()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "rbDbi5R9D30C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoches_loss = []"
      ],
      "metadata": {
        "id": "BlSl1fIBEIZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ckpt(checkpoint_fpath, model, optimizer):\n",
        "    \n",
        "    # load check point\n",
        "    checkpoint = torch.load(checkpoint_fpath)\n",
        "\n",
        "    # initialize state_dict from checkpoint to model\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # initialize optimizer from checkpoint to optimizer\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    # initialize valid_loss_min from checkpoint to valid_loss_min\n",
        "    #valid_loss_min = checkpoint['valid_loss_min']\n",
        "\n",
        "    # return model, optimizer, epoch value, min validation loss \n",
        "    return model, optimizer, checkpoint['epoch']\n",
        "\n",
        "def save_checkpoint(state, filename):\n",
        "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
        "    print (\"=> Saving a new best\")\n",
        "    torch.save(state, filename)  # save checkpoint\n",
        "    \n",
        "def train_model(model,  \n",
        "                criterion, \n",
        "                optimizer, \n",
        "                #scheduler, \n",
        "                num_epochs):\n",
        "    since = time.time()\n",
        "    \n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = np.inf\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for idx,inputs in enumerate(Bar(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, inputs)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "            #if phase == 'train':\n",
        "            #    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoches_loss.append(epoch_loss)\n",
        "            print('{} Loss: {:.4f}'.format(\n",
        "                phase, epoch_loss))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_loss < best_loss:\n",
        "                best_loss = epoch_loss\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                save_checkpoint(state={   \n",
        "                                    'epoch': epoch,\n",
        "                                    'state_dict': model.state_dict(),\n",
        "                                    'best_loss': best_loss,\n",
        "                                    'optimizer_state_dict':optimizer.state_dict()\n",
        "                                },filename='ckpt_epoch_{}.pt'.format(epoch))\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, optimizer, epoch_loss"
      ],
      "metadata": {
        "id": "Ipa1ZhVfER7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 2\n",
        "NUM_BATCHES = 4\n",
        "RETRAIN = False\n",
        "\n",
        "train_set, validate_set = prepare_data(DF=df)\n",
        "\n",
        "dataloaders = {'train': DataLoader(train_set, batch_size=NUM_BATCHES, shuffle=True, num_workers=1) ,\n",
        "                'val':DataLoader(validate_set, batch_size=NUM_BATCHES, num_workers=1)\n",
        "                }\n",
        "\n",
        "dataset_sizes = {'train': len(train_set),'val':len(validate_set)}\n",
        "\n",
        "model = ConvAutoencoder_v2().to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
      ],
      "metadata": {
        "id": "_cVOzPeCEmmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if RETRAIN == True:\n",
        "    model, optimizer, start_epoch = load_ckpt('/content/drive/MyDrive/datasets/ddt/conv_autoencoder.pt', model, optimizer)\n",
        "    print('Checkpoint Loaded')"
      ],
      "metadata": {
        "id": "CYtjzN2WEszv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, optimizer, loss = train_model(model=model, \n",
        "                    criterion=criterion, \n",
        "                    optimizer=optimizer,\n",
        "                    num_epochs=EPOCHS)"
      ],
      "metadata": {
        "id": "gjB-P7RJFajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(epoches_loss)"
      ],
      "metadata": {
        "id": "sed1-rkTgx61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_lss = []\n",
        "val_lss = []\n",
        "itr = []\n",
        "\n",
        "for i in range(len(epoches_loss)) :\n",
        "    if i%2==0 :\n",
        "        train_lss.append(epoches_loss[i])\n",
        "    else :\n",
        "        val_lss.append(epoches_loss[i])\n",
        "        \n",
        "for i in range(EPOCHS):\n",
        "    itr.append(i+1)\n",
        "        \n",
        "print(train_lss)\n",
        "print(val_lss)"
      ],
      "metadata": {
        "id": "kOrqBNz0hpA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(itr, train_lss, label = \"Training Loss\")\n",
        "\n",
        "# plotting the line 2 points\n",
        "plt.plot(itr, val_lss, label = \"Validation Loss\")\n",
        " \n",
        "plt.xlabel('Epoches')\n",
        "# naming the y axis\n",
        "plt.ylabel('Loss')\n",
        "# giving a title to my graph\n",
        "plt.title('Loss V/S Epoches')\n",
        " \n",
        "# show a legend on the plot\n",
        "plt.legend()\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PqHzNh7Gh2Zj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'epoch': EPOCHS,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'conv_autoencoderv2_200ep.pt')"
      ],
      "metadata": {
        "id": "YLsvKGqQiGp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformations = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])"
      ],
      "metadata": {
        "id": "OE9AZgMbiSKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latent_features(images, transformations):\n",
        "    \n",
        "    latent_features = np.zeros((4800,256,8,8))\n",
        "    \n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image).convert('RGB')).to(device)\n",
        "        latent_features[i] = model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy()\n",
        "        \n",
        "    gc.collect()\n",
        "    return latent_features"
      ],
      "metadata": {
        "id": "s8meQf-XkNYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvAutoencoder_v2().to(device)\n",
        "model.load_state_dict(torch.load('./conv_autoencoderv2_200ep.pt', map_location=device)['model_state_dict'], strict=False)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "PtHhfYxbic6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = df.image.values\n",
        "latent_features = get_latent_features(images, transformations)"
      ],
      "metadata": {
        "id": "QfAr3dSrkkf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = list(range(0, 4800))\n",
        "feature_dict = dict(zip(indexes,latent_features))\n",
        "index_dict = {'indexes':indexes,'features':latent_features}"
      ],
      "metadata": {
        "id": "l6_Zf5REleXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean(a, b):\n",
        "    # compute and return the euclidean distance between two vectors\n",
        "    return np.linalg.norm(a - b)"
      ],
      "metadata": {
        "id": "uhznRblrlwtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_distance(a,b):\n",
        "    return scipy.spatial.distance.cosine(a, b)"
      ],
      "metadata": {
        "id": "QdaZ2mzDl3GE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_search(queryFeatures, index, maxResults=64):\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for i in range(0, len(index[\"features\"])):\n",
        "        # compute the euclidean distance between our query features\n",
        "        # and the features for the current image in our index, then\n",
        "        # update our results list with a 2-tuple consisting of the\n",
        "        # computed distance and the index of the image\n",
        "        d = euclidean(queryFeatures, index[\"features\"][i])\n",
        "        results.append((d, i))\n",
        "    \n",
        "    # sort the results and grab the top ones\n",
        "    results = sorted(results)[:maxResults]\n",
        "    # return the list of results\n",
        "    return results"
      ],
      "metadata": {
        "id": "qEPgKcWXmCKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_montages(image_list, image_shape, montage_shape):\n",
        "\n",
        "    if len(image_shape) != 2:\n",
        "        raise Exception('image shape must be list or tuple of length 2 (rows, cols)')\n",
        "    if len(montage_shape) != 2:\n",
        "        raise Exception('montage shape must be list or tuple of length 2 (rows, cols)')\n",
        "    image_montages = []\n",
        "    montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                          dtype=np.uint8)\n",
        "    cursor_pos = [0, 0]\n",
        "    start_new_img = False\n",
        "    for img in image_list:\n",
        "        if type(img).__module__ != np.__name__:\n",
        "            raise Exception('input of type {} is not a valid numpy array'.format(type(img)))\n",
        "        start_new_img = False\n",
        "        img = cv2.resize(img, image_shape)\n",
        "        montage_image[cursor_pos[1]:cursor_pos[1] + image_shape[1], cursor_pos[0]:cursor_pos[0] + image_shape[0]] = img\n",
        "        cursor_pos[0] += image_shape[0]  # increment cursor x position\n",
        "        if cursor_pos[0] >= montage_shape[0] * image_shape[0]:\n",
        "            cursor_pos[1] += image_shape[1]  # increment cursor y position\n",
        "            cursor_pos[0] = 0\n",
        "            if cursor_pos[1] >= montage_shape[1] * image_shape[1]:\n",
        "                cursor_pos = [0, 0]\n",
        "                image_montages.append(montage_image)\n",
        "                # reset black canvas\n",
        "                montage_image = np.zeros(shape=(image_shape[1] * (montage_shape[1]), image_shape[0] * montage_shape[0], 3),\n",
        "                                      dtype=np.uint8)\n",
        "                start_new_img = True\n",
        "    if start_new_img is False:\n",
        "        image_montages.append(montage_image)  # add unfinished montage\n",
        "    return image_montages"
      ],
      "metadata": {
        "id": "3NNYpy8kmPRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "queryIdx = 16# Input Index for which images \n",
        "MAX_RESULTS = 25\n",
        "\n",
        "\n",
        "queryFeatures = latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    print(j)\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(images[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (10, 3))[0]\n",
        "ax[1].imshow(montage)"
      ],
      "metadata": {
        "id": "CONjKHmnmsDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testpath = Path('/content/drive/MyDrive/datasets/xx/')\n",
        "testdf = pd.DataFrame()\n",
        "\n",
        "testdf['image'] = [f for f in os.listdir(testpath) if os.path.isfile(os.path.join(testpath, f))]\n",
        "testdf['image'] = '/content/drive/MyDrive/datasets/xx/' + testdf['image'].astype(str)\n",
        "\n",
        "testdf.head()"
      ],
      "metadata": {
        "id": "5-zfglu8nVzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testimages = testdf.image.values\n",
        "test_latent_features = get_latent_features(testimages, transformations)"
      ],
      "metadata": {
        "id": "UE5JiL6cnreF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(nrows=2,figsize=(15,15))\n",
        "MAX_RESULTS = 14\n",
        "queryIdx =5\n",
        "\n",
        "queryFeatures = test_latent_features[queryIdx]\n",
        "results = perform_search(queryFeatures, index_dict, maxResults=MAX_RESULTS)\n",
        "imgs = []\n",
        "\n",
        "# loop over the results\n",
        "for (d, j) in results:\n",
        "    img = np.array(Image.open(images[j]))\n",
        "    print(j)\n",
        "    imgs.append(img)\n",
        "\n",
        "# display the query image\n",
        "ax[0].imshow(np.array(Image.open(testimages[queryIdx])))\n",
        "\n",
        "# build a montage from the results and display it\n",
        "montage = build_montages(imgs, (512, 512), (5, 3))[0]\n",
        "ax[1].imshow(montage)"
      ],
      "metadata": {
        "id": "9gDyyronnzQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_latent_features.shape"
      ],
      "metadata": {
        "id": "PSBv5lAwsbAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "\n",
        "def get_latent_features1D(images, transformations):\n",
        "    \n",
        "    latent_features1d = []\n",
        "    \n",
        "    for i,image in enumerate(tqdm(images)):\n",
        "        tensor = transformations(Image.open(image)).to(device)\n",
        "        latent_features1d.append(model.encoder(tensor.unsqueeze(0)).cpu().detach().numpy().flatten())\n",
        "        \n",
        "    del tensor\n",
        "    gc.collect()\n",
        "    return latent_features1d\n",
        "images = df.image.values\n",
        "latent_features1d = get_latent_features1D(images, transformations)\n",
        "latent_features1d = np.array(latent_features1d)\n",
        "distortions = [] \n",
        "inertias = [] \n",
        "mapping1 = {} \n",
        "mapping2 = {} \n",
        "K = range(4,10) \n",
        "  \n",
        "for k in tqdm(K): \n",
        "    #Building and fitting the model \n",
        "    kmeanModel = KMeans(n_clusters=k).fit(latent_features1d)      \n",
        "      \n",
        "    distortions.append(sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n",
        "                      'euclidean'),axis=1)) / latent_features1d.shape[0]) \n",
        "    inertias.append(kmeanModel.inertia_) \n",
        "  \n",
        "    mapping1[k] = sum(np.min(cdist(latent_features1d, kmeanModel.cluster_centers_, \n",
        "                 'euclidean'),axis=1)) / latent_features1d.shape[0] \n",
        "    mapping2[k] = kmeanModel.inertia_ \n",
        "plt.plot(K, distortions, 'bx-') \n",
        "plt.xlabel('Values of K') \n",
        "plt.ylabel('Distortion') \n",
        "plt.title('The Elbow Method using Distortion') \n",
        "plt.show() \n",
        "X = np.array(latent_features1d)\n",
        "K = range(3,10) \n",
        "\n",
        "for n_clusters in tqdm(K):\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # The 1st subplot is the silhouette plot\n",
        "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=RANDOMSTATE)\n",
        "    cluster_labels = clusterer.fit_predict(X)\n",
        "\n",
        "    # The silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"The average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # The vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"The visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yTXr240AsjdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dictionary(xfeatures2d, images, n_clusters):\n",
        "    #print('Computing descriptors..')        \n",
        "    desc_list = []\n",
        "    \n",
        "    for image_path in images:\n",
        "        image = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        kp, dsc = xfeatures2d.detectAndCompute(gray, None)\n",
        "        desc_list.extend(dsc)\n",
        "\n",
        "    desc = np.array(desc_list)\n",
        "    #print('Creating BoW dictionary using K-Means clustering with k={}..'.format(n_clusters))\n",
        "    dictionary = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, verbose=0)\n",
        "    dictionary.fit(desc)\n",
        "    \n",
        "    distortion = sum(np.min(cdist(desc, dictionary.cluster_centers_, \n",
        "                      'euclidean'),axis=1)) / desc.shape[0]\n",
        "    \n",
        "    return distortion"
      ],
      "metadata": {
        "id": "YrcJIZFnuOtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "F8VKBftW0b9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orb = cv2.ORB_create()\n",
        "images = df.image.values\n",
        "K = range(4,10)\n",
        "distortions = []\n",
        "\n",
        "for k in tqdm(K):\n",
        "    distortions.append(build_dictionary(orb, images, n_clusters=k))"
      ],
      "metadata": {
        "id": "3o3XAiZZvS6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(K, distortions, 'bx-') \n",
        "plt.xlabel('Values of K') \n",
        "plt.ylabel('Distortion') \n",
        "plt.title('The Elbow Method using Distortion') \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OI-61mEDvy-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dictionary(xfeatures2d, images, n_clusters):\n",
        "    #print('Computing descriptors..')        \n",
        "    desc_list = []\n",
        "    \n",
        "    for image_path in images:\n",
        "        image = cv2.imread(image_path)\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        kp, dsc = xfeatures2d.detectAndCompute(gray, None)\n",
        "        desc_list.extend(dsc)\n",
        "\n",
        "    desc = np.array(desc_list)\n",
        "    #print('Creating BoW dictionary using K-Means clustering with k={}..'.format(n_clusters))\n",
        "    dictionary = MiniBatchKMeans(n_clusters=n_clusters, batch_size=100, verbose=0)\n",
        "    dictionary.fit(desc)\n",
        "    \n",
        "    distortion = sum(np.min(cdist(desc, dictionary.cluster_centers_, \n",
        "                      'euclidean'),axis=1)) / desc.shape[0]\n",
        "    \n",
        "    return distortion"
      ],
      "metadata": {
        "id": "5p2V-coP0epM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orb = cv2.ORB_create()\n",
        "images = df.image.values\n",
        "K = range(5,10)\n",
        "distortions = []\n",
        "\n",
        "for k in tqdm(K):\n",
        "    distortions.append(build_dictionary(orb, images, n_clusters=k))"
      ],
      "metadata": {
        "id": "kNAewfIy0pmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}